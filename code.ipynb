{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AI\\NN\\airflow\\aflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">c:\\AI\\NN\\airflow\\aflow\\lib\\site-packages\\airflow\\configuration.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">750</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> UserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Config scheduler.max_tis_per_query </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">value: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">512</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\"> should NOT be greater than core.parallelism </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">value: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">32</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">. Will now use core.parallelism as the max task instances per query instead of specified value.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mc:\\AI\\NN\\airflow\\aflow\\lib\\site-packages\\airflow\\configuration.py:\u001b[0m\u001b[1;33m750\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Config scheduler.max_tis_per_query \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mvalue: \u001b[0m\u001b[1;33m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m should NOT be greater than core.parallelism \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mvalue: \u001b[0m\u001b[1;33m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m. Will now use core.parallelism as the max task instances per query instead of specified value.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:OSError while attempting to symlink the latest log directory\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dtm\n",
    "import time\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pymongo\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "\n",
    "    def __init__(self, url, path):\n",
    "        self.url = url\n",
    "        self.path = path\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_accents(text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('á', 'a')\n",
    "        text = text.replace('é', 'e')\n",
    "        text = text.replace('í', 'i')\n",
    "        text = text.replace('ó', 'o')\n",
    "        text = text.replace('ú', 'u')\n",
    "        translator = str.maketrans('', '', '¿?¡!(),.;:«»\"“”…–-—’‘’')\n",
    "        text = text.translate(translator)\n",
    "        return text\n",
    "\n",
    "    def scrape(self):\n",
    "        pedido_obtenido = requests.get(self.url)\n",
    "        pedido_obtenido.encoding = 'utf-8'\n",
    "        html_obtenido = pedido_obtenido.text\n",
    "        soup = BeautifulSoup(html_obtenido, 'html.parser')\n",
    "        news = []\n",
    "        h3_todos = soup.find_all('h3')\n",
    "        for h3 in h3_todos:\n",
    "            texto = h3.text.strip()\n",
    "            texto = self.remove_accents(texto)\n",
    "            news.append(texto)\n",
    "        return news\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        date =  dtm.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        news = self.scrape()\n",
    "        df = pd.DataFrame(np.array(news), columns=['news'])\n",
    "        df.to_excel(self.path + 'news_' + str(date) + '.xlsx')\n",
    "        return \"Archivo creado\"\n",
    "\n",
    "    def main(self):\n",
    "        news = self.scrape()\n",
    "        result = self.create_dataframe()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datanews(url):\n",
    "    scraper = WebScraper(url, 'data/')\n",
    "    result = scraper.main()\n",
    "    print(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #url = 'https://www.elcolombiano.com'\n",
    "    generate_datanews(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">c:\\AI\\NN\\airflow\\aflow\\lib\\site-packages\\transformers\\trainer_tf.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">118</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> FutureWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The class `TFTrainer` is deprecated and will be removed in version </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000\"> of Transformers. We recommend using native Keras instead, by calling methods like `</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">fit()</span><span style=\"color: #808000; text-decoration-color: #808000\">` and `</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">predict()</span><span style=\"color: #808000; text-decoration-color: #808000\">` directly on the model object. Detailed examples of the Keras style can be found in our examples at </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://github.com/huggingface/transformers/tree/main/examples/tensorflow</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mc:\\AI\\NN\\airflow\\aflow\\lib\\site-packages\\transformers\\trainer_tf.py:\u001b[0m\u001b[1;33m118\u001b[0m\u001b[1;33m FutureWarning\u001b[0m\u001b[33m: The class `TFTrainer` is deprecated and will be removed in version \u001b[0m\u001b[1;33m5\u001b[0m\u001b[33m of Transformers. We recommend using native Keras instead, by calling methods like `\u001b[0m\u001b[1;33mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m` and `\u001b[0m\u001b[1;33mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m` directly on the model object. Detailed examples of the Keras style can be found in our examples at \u001b[0m\u001b[4;33mhttps://github.com/huggingface/transformers/tree/main/examples/tensorflow\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  {'eval_loss': 1.0505685806274414}\n"
     ]
    }
   ],
   "source": [
    "class TextClassification:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def data_reading(self):\n",
    "        df = pd.read_excel('data/news.xlsx', sheet_name='Sheet1')\n",
    "        df['encoded_tag'] = df['tag'].map({'negativa': 0, 'neutra': 1, 'positiva': 2})\n",
    "        data_texts = df['news'].to_list()\n",
    "        data_labels = df['encoded_tag'].to_list()\n",
    "        return data_texts, data_labels\n",
    "\n",
    "    def train_test_split(self, data_texts, data_labels):\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0)\n",
    "        train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0)\n",
    "        return train_texts, val_texts, test_texts, train_labels, val_labels, test_labels\n",
    "\n",
    "    def tokenization(self, train_texts, val_texts, train_labels, val_labels):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        train_encodings = self.tokenizer(train_texts, truncation=True, padding=True)\n",
    "        val_encodings = self.tokenizer(val_texts, truncation=True, padding=True)\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(train_encodings),\n",
    "            train_labels\n",
    "        ))\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(val_encodings),\n",
    "            val_labels\n",
    "        ))\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def modeling(self, train_dataset, val_dataset):\n",
    "        self.model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "        training_args = TFTrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=7,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=64,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=1e-5,\n",
    "            logging_dir='./logs',\n",
    "            eval_steps=100\n",
    "        )\n",
    "\n",
    "        with training_args.strategy.scope():\n",
    "            trainer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "        trainer = TFTrainer(\n",
    "            model=trainer_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "        )\n",
    "        trainer.train()\n",
    "        accuracy = trainer.evaluate()\n",
    "        save_directory = \"/saved_models\"\n",
    "        self.model.save_pretrained(save_directory)\n",
    "        self.tokenizer.save_pretrained(save_directory)\n",
    "        return accuracy, save_directory\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text_classifier = TextClassification()\n",
    "    data_texts, data_labels = text_classifier.data_reading()\n",
    "    train_texts, val_texts, test_texts, train_labels, val_labels, test_labels = text_classifier.train_test_split(data_texts, data_labels)\n",
    "    train_dataset, val_dataset = text_classifier.tokenization(train_texts, val_texts, train_labels, val_labels)\n",
    "    accuracy, save_dir = text_classifier.modeling(train_dataset, val_dataset)\n",
    "    print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /saved_models were not used when initializing TFDistilBertForSequenceClassification: ['dropout_279']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /saved_models and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 la reina del tiro con arco la colombiana sara lopez se corono campeona del mundial de tiro con arco\n"
     ]
    }
   ],
   "source": [
    "class ModelLoader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"/saved_models\", verbose=False)\n",
    "        self.model = TFDistilBertForSequenceClassification.from_pretrained(\"/saved_models\")\n",
    "\n",
    "    def load_model(self, test_texts):\n",
    "        predict_input = self.tokenizer.encode(\n",
    "            test_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        output = self.model(predict_input)[0]\n",
    "        prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "        return prediction_value, test_texts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_excel('data/news.xlsx', sheet_name='Sheet1')\n",
    "    data_texts = df['news'].to_list()[7]\n",
    "    model_loader = ModelLoader()\n",
    "    prediction_value, test_texts = model_loader.load_model(data_texts)\n",
    "    print(prediction_value, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /saved_models were not used when initializing TFDistilBertForSequenceClassification: ['dropout_199']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /saved_models and are newly initialized: ['dropout_259']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:  la reina del tiro con arco la colombiana sara lopez se corono campeona del mundial de tiro con arco\n",
      "Prediction:  1\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    df = pd.read_excel('data/news.xlsx', sheet_name='Sheet1')\n",
    "    data_texts = df['news'].to_list()[7]\n",
    "    model_loader = ModelLoader()\n",
    "    prediction_value, text = model_loader.load_model(data_texts)\n",
    "    print('New: ', text)\n",
    "    print('Prediction: ', prediction_value)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /saved_models were not used when initializing TFDistilBertForSequenceClassification: ['dropout_279']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /saved_models and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class NewsPredictor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict_and_save_to_excel(self):\n",
    "        prediction_list = []\n",
    "        date = dtm.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        df = pd.read_excel(f'news/news_{date}.xlsx', sheet_name='Sheet1')\n",
    "        model_loader = ModelLoader()\n",
    "        for text in df['news'].to_list():\n",
    "            prediction_value, text = model_loader.load_model(text)\n",
    "            prediction_list.append(prediction_value)\n",
    "        df['prediction'] = prediction_list\n",
    "        df.to_excel(f'news/news_{date}.xlsx')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    predictor = NewsPredictor()\n",
    "    predictor.predict_and_save_to_excel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client['News_colombiano']\n",
    "collection = db.prediction_collection\n",
    "\n",
    "class DataBase:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def savedata(self):\n",
    "        date = dtm.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        news = pd.read_excel(f'news/news_{date}.xlsx', sheet_name='Sheet1')\n",
    "        news = news.to_dict('records')\n",
    "        collection.insert_many(news)\n",
    "        return \"Data saved\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    database = DataBase()\n",
    "    database.savedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReporter:\n",
    "    def __init__(self, news, email, password):\n",
    "        self.news = news\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "\n",
    "    def report(self):\n",
    "        negative_news = self.news[self.news['prediction'] == 0]\n",
    "        positive_news = self.news[self.news['prediction'] == 2]\n",
    "        negative_news = negative_news['news'].to_list()\n",
    "        positive_news = positive_news['news'].to_list()\n",
    "        negative_news = '\\n'.join(negative_news)\n",
    "        positive_news = '\\n'.join(positive_news)\n",
    "        return {\n",
    "            'negative_news': negative_news,\n",
    "            'positive_news': positive_news\n",
    "        }\n",
    "\n",
    "    def send_email(self):\n",
    "        date = dtm.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        df = pd.read_excel(f'news/news_{date}.xlsx', sheet_name='Sheet1')\n",
    "        reporte = self.report()\n",
    "        smtp_server = 'smtp.gmail.com'\n",
    "        smtp_port = 587\n",
    "        smtp_username = self.email\n",
    "        smtp_password = self.password\n",
    "\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = smtp_username\n",
    "        msg['To'] = to_email\n",
    "        msg['Subject'] = 'Informe de noticias'\n",
    "\n",
    "        body = f'''\n",
    "        Noticias Negativas:\n",
    "        {reporte['negative_news']}\n",
    "\n",
    "        Noticias Positivas:\n",
    "        {reporte['positive_news']}\n",
    "        '''\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        try:\n",
    "            server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "            server.starttls()\n",
    "            server.login(smtp_username, smtp_password)\n",
    "            server.sendmail(smtp_username, to_email, msg.as_string())\n",
    "            server.quit()\n",
    "            print(\"Correo enviado con éxito\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al enviar el correo: {str(e)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    date = dtm.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    path = '../news/' + 'news_' + str(date) + '.xlsx'\n",
    "    to_email = 'sergio.quintero.1804@gmail.com'\n",
    "    password = 'password_dummy'\n",
    "    visualizer = DataReporter(path, to_email, password)\n",
    "    visualizer.send_email(to_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_18756\\</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">438768565.</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">8</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mC:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_18756\\\u001b[0m\u001b[1;33m438768565.\u001b[0m\u001b[1;33mpy:\u001b[0m\u001b[1;33m8\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_args = {\n",
    "    'owner': 'tu_nombre',\n",
    "    'start_date': datetime(2023, 9, 12, 6, 0, 0),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'web_scraping_and_prediction',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "def run_web_scraper():\n",
    "    WebScraper()\n",
    "\n",
    "web_scraping_task = PythonOperator(\n",
    "    task_id='web_scraping_task',\n",
    "    python_callable=run_web_scraper,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "def run_prediction():\n",
    "    NewsPredictor()\n",
    "\n",
    "prediction_task = PythonOperator(\n",
    "    task_id='prediction_task',\n",
    "    python_callable=run_prediction,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "def run_database():\n",
    "    DataBase()\n",
    "\n",
    "database_task = PythonOperator(\n",
    "    task_id='database_task',\n",
    "    python_callable=run_database,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "def run_report():\n",
    "    DataReporter()\n",
    "\n",
    "report_task = PythonOperator(\n",
    "    task_id='report_task',\n",
    "    python_callable=run_report,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "web_scraping_task >> prediction_task  >> database_task >> report_task\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag.cli()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
